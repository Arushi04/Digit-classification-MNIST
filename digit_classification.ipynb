{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems. It is a collection of 70000 handwritten digits split into training and test set of 60000 and 10000 images respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Converting the image into torch tensor : First the images are converted to numbers and then separated into\n",
    "RGB color channels. The image pixels are then converted to lie between 0 and 255. These values are then \n",
    "scaled down to lie between 0 and 1'''\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''As the dataset consists of no validation set, we will be splitting the training data into \n",
    "train and validation set. We will keep 20% of the training set as the validation set.'''\n",
    "\n",
    "#Downloading the dataset\n",
    "trainset = datasets.MNIST('.', download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST('.', download=True, train=False, transform=transform)\n",
    "\n",
    "batch_size = 64  #no. of images we want to read in one go.\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(trainset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "\n",
    "#Loading the dataset\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=val_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set :  48000\n",
      "validation set :  12000\n",
      "test set :  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"train set : \", len(train_sampler))\n",
    "print(\"validation set : \", len(val_sampler))\n",
    "print(\"test set : \", len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.linear(x)\n",
    "        return pred\n",
    "    \n",
    "#Defining the function to calculate accuracy\n",
    "\n",
    "def calc_accuracy(pred, label):\n",
    "    max_vals, max_indices = torch.max(pred, 1)\n",
    "    train_acc = (max_indices == label).sum().item()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(\n",
      "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "print(model)\n",
    "\n",
    "#Defining loss function and updating through gradient descent\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  #combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "epoch_no = 30\n",
    "writer = SummaryWriter(log_dir=\"./runs/lr_0.01\")\n",
    "for epoch in range(epoch_no):\n",
    "    epoch_train_loss = 0\n",
    "    running_accuracy = 0\n",
    "    \n",
    "    model.train()     # training the model\n",
    "    for images, label in train_loader:\n",
    "        features = torch.flatten(images, start_dim=1)\n",
    "        optimizer.zero_grad()  # setting the gradient to zero else they get accumulated\n",
    "        pred = model(batch_size * features)   # forward pass : calls the forward method defined in model\n",
    "        train_loss = criterion(pred, label)  # computing loss using cross entropy\n",
    "        train_loss.backward()        # backward pass : calculating the gradient\n",
    "        optimizer.step()       # updating the theta/parameter\n",
    "        epoch_train_loss += train_loss.item()\n",
    "    epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "    \n",
    "    \n",
    "    running_val_loss = 0\n",
    "    model.eval()\n",
    "    for images, label in val_loader:\n",
    "        features = torch.flatten(images, start_dim=1)\n",
    "        pred = model(features)   # forward pass\n",
    "        val_loss = criterion(pred, label)\n",
    "        accuracy = calc_accuracy(pred, label)\n",
    "        running_val_loss += val_loss.item()\n",
    "        running_accuracy += accuracy  \n",
    "    running_val_loss = running_val_loss/len(val_loader) \n",
    "    avg_accuracy = running_accuracy/len(val_loader)\n",
    "    \n",
    "    #print(f\"Epoch {epoch+1} : Train Loss : {epoch_train_loss} & Val Loss : {running_val_loss},  Val accuracy : {avg_accuracy}\")\n",
    "    \n",
    "    writer.add_scalar('Loss/train', epoch_train_loss, epoch+1)\n",
    "    writer.add_scalar('Loss/validation', running_val_loss, epoch+1)\n",
    "    writer.add_scalar('Accuracy/validation', avg_accuracy, epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
